{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 18 15:58:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.57                 Driver Version: 576.57         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P5              4W /   80W |     124MiB /   8188MiB |     42%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           14244    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.2.0+cu121\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp39-cp39-win_amd64.whl (2454.8 MB)\n",
      "Collecting torchvision==0.17.0+cu121\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp39-cp39-win_amd64.whl (5.7 MB)\n",
      "Collecting torchaudio==2.2.0\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "Collecting filelock (from torch==2.2.0+cu121)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in .\\.venv\\lib\\site-packages (from torch==2.2.0+cu121) (4.12.2)\n",
      "Collecting sympy (from torch==2.2.0+cu121)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.2.0+cu121)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch==2.2.0+cu121)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch==2.2.0+cu121)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting numpy (from torchvision==0.17.0+cu121)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Collecting requests (from torchvision==0.17.0+cu121)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.17.0+cu121)\n",
      "  Using cached pillow-11.1.0-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.2.0+cu121)\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl.metadata (4.1 kB)\n",
      "INFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting networkx (from torch==2.2.0+cu121)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchvision==0.17.0+cu121)\n",
      "  Using cached charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision==0.17.0+cu121)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchvision==0.17.0+cu121)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision==0.17.0+cu121)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.0+cu121)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pillow-11.1.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: mpmath, urllib3, sympy, pillow, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 fsspec-2025.3.0 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 pillow-11.1.0 requests-2.32.3 sympy-1.13.3 torch-2.2.0+cu121 torchaudio-2.2.0+cu121 torchvision-0.17.0+cu121 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.2.0+cu121 torchvision==0.17.0+cu121 torchaudio==2.2.0 --extra-index-url https://download.pytorch.org/whl/cu121 for 4090 or below\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128  for 5090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Change this line to switch between GPU and CPU manually\n",
    "use_gpu = True  # Set False to force CPU\n",
    "\n",
    "device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0+cu121\n",
      "CUDA available: True\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Current Device: 0\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Check the GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current Device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Matrix multiplication time on GPU: 0.2937 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get GPU details\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Test performance with a simple matrix multiplication\n",
    "    tensor1 = torch.randn(10000, 10000, device=device)\n",
    "    tensor2 = torch.randn(10000, 10000, device=device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = torch.matmul(tensor1, tensor2)  # Perform matrix multiplication on GPU\n",
    "    torch.cuda.synchronize()  # Ensure all CUDA operations are completed\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Matrix multiplication time on GPU: {end_time - start_time:.4f} seconds\")\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch CUDA Version:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 17 22:18:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.70                 Driver Version: 572.70         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   37C    P8              1W /   80W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           18420    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           22064    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           24380    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
